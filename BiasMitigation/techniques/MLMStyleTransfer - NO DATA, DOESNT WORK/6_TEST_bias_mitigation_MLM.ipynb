{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-SXM3-32GB\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from sentence_transformers import util\n",
    "\n",
    "\n",
    "# random.seed(42)\n",
    "torch.manual_seed(42) \n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "  print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "  print('No GPU available, using the CPU instead.')\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################### Load dataset ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"path_to/female_only.test.en\", header=None)\n",
    "X_test = X_test[0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################### load fully trained models ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"mitigation_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sbert.load_state_dict(torch.load(\"generate_neutral_latent_embedding_model.pth\").bias_mitigation_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_detector = torch.load(\"bias_class_discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c90470ecf5c42f580e288c663cb63e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23856eff7f364c3a9e8bbcf0ed3a42a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6beb85944ad4e98b036499d072b9e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=391.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7adc5b113a4a04af4783af517ace10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3951.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e66e8b0c664f89b3c583db9aada10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead027b972be421f9eefc685d19bf1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b7e94536c1464a9c60b19f1e4cb9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=122.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fe92eb61f24fc49ed5842898c654ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=229.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4695c0817e44dcc8525b33c9c494666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=438007537.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab440f3274f4836b38027796ac0aa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=53.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980865263ee943f1aa858b41f7ddad38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa767e5a1db48f4bd21012e9c66e39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466081.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5217ce9a393b46b6ada0b1c827288d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=399.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159dd1af0f784695973a4b1088d44546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08165998111e43c59b4d6da3d0b045b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=190.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing SBERT: ['bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'cls.predictions.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing SBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.sbert.encoder.layer.9.intermediate.dense.bias', 'bert.sbert.encoder.layer.11.intermediate.dense.bias', 'bert.sbert.encoder.layer.1.intermediate.dense.bias', 'bert.sbert.encoder.layer.5.output.LayerNorm.bias', 'bert.sbert.encoder.layer.7.output.dense.weight', 'bert.sbert.encoder.layer.11.intermediate.dense.weight', 'bert.sbert.encoder.layer.8.attention.self.query.bias', 'bert.sbert.encoder.layer.9.output.dense.weight', 'bert.sbert.encoder.layer.8.output.dense.weight', 'bert.sbert.encoder.layer.7.attention.self.query.weight', 'bert.sbert.embeddings.LayerNorm.weight', 'bert.sbert.encoder.layer.0.attention.self.value.bias', 'bert.sbert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.sbert.encoder.layer.0.output.dense.weight', 'bert.sbert.encoder.layer.3.output.LayerNorm.weight', 'bert.sbert.encoder.layer.11.attention.output.dense.bias', 'bert.sbert.encoder.layer.4.attention.self.key.weight', 'bert.sbert.encoder.layer.0.output.LayerNorm.weight', 'bert.sbert.encoder.layer.2.attention.output.dense.bias', 'bert.sbert.encoder.layer.4.intermediate.dense.bias', 'bert.sbert.encoder.layer.0.attention.self.query.bias', 'bert.sbert.encoder.layer.7.attention.output.dense.bias', 'bert.sbert.encoder.layer.9.attention.self.key.weight', 'bert.sbert.encoder.layer.11.output.dense.weight', 'bert.sbert.encoder.layer.2.attention.self.value.weight', 'bert.sbert.encoder.layer.3.output.dense.weight', 'bert.sbert.encoder.layer.1.output.LayerNorm.weight', 'bert.sbert.encoder.layer.3.attention.self.value.bias', 'bert.sbert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.sbert.encoder.layer.10.attention.output.dense.weight', 'bert.sbert.encoder.layer.5.output.dense.weight', 'bert.sbert.encoder.layer.3.intermediate.dense.bias', 'bert.sbert.encoder.layer.1.attention.self.key.weight', 'bert.sbert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.sbert.encoder.layer.7.attention.output.dense.weight', 'bert.sbert.encoder.layer.1.attention.output.dense.weight', 'bert.sbert.encoder.layer.11.attention.output.dense.weight', 'bert.sbert.encoder.layer.0.intermediate.dense.weight', 'bert.sbert.encoder.layer.3.output.dense.bias', 'bert.sbert.encoder.layer.9.output.LayerNorm.weight', 'bert.sbert.encoder.layer.3.intermediate.dense.weight', 'bert.sbert.encoder.layer.10.output.LayerNorm.weight', 'bert.sbert.encoder.layer.1.attention.self.value.weight', 'bert.sbert.encoder.layer.6.attention.self.query.weight', 'bert.sbert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.sbert.encoder.layer.10.intermediate.dense.weight', 'bert.sbert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.sbert.encoder.layer.2.intermediate.dense.bias', 'bert.sbert.encoder.layer.10.output.dense.weight', 'bert.sbert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.sbert.encoder.layer.7.intermediate.dense.bias', 'bert.sbert.encoder.layer.6.attention.self.key.bias', 'bert.sbert.encoder.layer.9.attention.self.value.bias', 'bert.sbert.encoder.layer.7.attention.self.value.bias', 'bert.sbert.encoder.layer.8.intermediate.dense.bias', 'bert.sbert.encoder.layer.9.attention.self.query.weight', 'bert.sbert.pooler.dense.weight', 'bert.sbert.pooler.dense.bias', 'bert.sbert.encoder.layer.7.attention.self.query.bias', 'bert.sbert.encoder.layer.5.attention.self.key.bias', 'bert.sbert.encoder.layer.4.output.LayerNorm.weight', 'bert.sbert.encoder.layer.10.attention.self.value.weight', 'bert.sbert.encoder.layer.4.attention.output.dense.bias', 'bert.sbert.encoder.layer.4.attention.output.dense.weight', 'bert.sbert.encoder.layer.7.attention.self.value.weight', 'bert.sbert.encoder.layer.7.intermediate.dense.weight', 'bert.sbert.encoder.layer.0.attention.output.dense.weight', 'bert.sbert.encoder.layer.5.attention.self.value.bias', 'bert.sbert.encoder.layer.9.output.dense.bias', 'bert.sbert.encoder.layer.0.output.dense.bias', 'bert.sbert.encoder.layer.3.attention.self.query.weight', 'bert.sbert.embeddings.position_embeddings.weight', 'bert.sbert.encoder.layer.3.attention.self.query.bias', 'bert.sbert.encoder.layer.5.attention.output.dense.bias', 'bert.sbert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.sbert.encoder.layer.10.attention.self.query.bias', 'bert.sbert.encoder.layer.4.attention.self.query.bias', 'bert.sbert.encoder.layer.10.output.LayerNorm.bias', 'bert.sbert.encoder.layer.5.intermediate.dense.bias', 'bert.sbert.encoder.layer.0.intermediate.dense.bias', 'bert.sbert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.sbert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.sbert.encoder.layer.0.attention.self.key.bias', 'bert.sbert.encoder.layer.9.output.LayerNorm.bias', 'bert.sbert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.sbert.encoder.layer.10.output.dense.bias', 'bert.sbert.encoder.layer.8.output.LayerNorm.weight', 'bert.sbert.encoder.layer.8.attention.self.query.weight', 'bert.sbert.encoder.layer.10.attention.self.query.weight', 'bert.sbert.encoder.layer.7.output.LayerNorm.bias', 'bert.sbert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.sbert.encoder.layer.4.attention.self.value.bias', 'bert.sbert.encoder.layer.4.attention.self.key.bias', 'bert.sbert.encoder.layer.2.attention.output.dense.weight', 'bert.sbert.embeddings.LayerNorm.bias', 'bert.sbert.encoder.layer.3.attention.self.key.weight', 'bert.sbert.encoder.layer.3.output.LayerNorm.bias', 'bert.sbert.encoder.layer.6.output.LayerNorm.bias', 'bert.sbert.encoder.layer.5.intermediate.dense.weight', 'bert.sbert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.sbert.encoder.layer.2.attention.self.key.bias', 'bert.sbert.encoder.layer.2.attention.self.key.weight', 'bert.sbert.encoder.layer.2.attention.self.value.bias', 'bert.sbert.encoder.layer.5.attention.self.query.weight', 'bert.sbert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.sbert.encoder.layer.11.attention.self.query.weight', 'bert.sbert.encoder.layer.1.attention.self.value.bias', 'bert.sbert.encoder.layer.11.output.LayerNorm.weight', 'bert.sbert.encoder.layer.4.intermediate.dense.weight', 'bert.sbert.encoder.layer.6.output.dense.weight', 'bert.sbert.encoder.layer.5.attention.self.value.weight', 'bert.sbert.encoder.layer.8.output.dense.bias', 'bert.sbert.encoder.layer.7.attention.self.key.weight', 'bert.sbert.encoder.layer.11.attention.self.value.bias', 'bert.sbert.encoder.layer.11.attention.self.key.weight', 'bert.sbert.encoder.layer.6.attention.output.dense.bias', 'bert.sbert.encoder.layer.8.attention.self.key.bias', 'bert.sbert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.sbert.encoder.layer.7.output.dense.bias', 'bert.sbert.encoder.layer.6.attention.self.value.weight', 'bert.sbert.encoder.layer.10.attention.self.key.weight', 'bert.sbert.encoder.layer.0.output.LayerNorm.bias', 'bert.sbert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.sbert.encoder.layer.8.attention.output.dense.bias', 'bert.sbert.encoder.layer.1.intermediate.dense.weight', 'bert.sbert.encoder.layer.0.attention.self.value.weight', 'bert.sbert.encoder.layer.1.attention.self.query.bias', 'bert.sbert.encoder.layer.8.attention.self.value.bias', 'bert.sbert.encoder.layer.4.output.LayerNorm.bias', 'bert.sbert.encoder.layer.6.attention.self.value.bias', 'bert.sbert.encoder.layer.9.attention.output.dense.weight', 'bert.sbert.encoder.layer.4.attention.self.query.weight', 'bert.sbert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.sbert.encoder.layer.0.attention.self.key.weight', 'bert.sbert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.sbert.encoder.layer.4.output.dense.bias', 'bert.sbert.encoder.layer.11.attention.self.key.bias', 'bert.sbert.encoder.layer.4.attention.self.value.weight', 'bert.sbert.encoder.layer.9.attention.self.query.bias', 'bert.sbert.encoder.layer.0.attention.output.dense.bias', 'bert.sbert.encoder.layer.6.intermediate.dense.weight', 'bert.sbert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.sbert.encoder.layer.6.attention.self.query.bias', 'bert.sbert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.sbert.encoder.layer.9.attention.self.key.bias', 'bert.sbert.encoder.layer.1.attention.output.dense.bias', 'bert.sbert.encoder.layer.7.output.LayerNorm.weight', 'bert.sbert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.sbert.encoder.layer.10.intermediate.dense.bias', 'bert.sbert.encoder.layer.0.attention.self.query.weight', 'bert.sbert.encoder.layer.6.attention.self.key.weight', 'bert.sbert.encoder.layer.8.attention.self.key.weight', 'bert.sbert.encoder.layer.6.output.dense.bias', 'bert.sbert.encoder.layer.2.output.dense.weight', 'bert.sbert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.sbert.encoder.layer.4.output.dense.weight', 'bert.sbert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.sbert.encoder.layer.1.attention.self.key.bias', 'bert.sbert.encoder.layer.5.attention.output.dense.weight', 'bert.sbert.encoder.layer.2.attention.self.query.weight', 'bert.sbert.encoder.layer.6.output.LayerNorm.weight', 'bert.sbert.encoder.layer.11.output.dense.bias', 'bert.sbert.encoder.layer.2.intermediate.dense.weight', 'bert.sbert.encoder.layer.6.attention.output.dense.weight', 'bert.sbert.encoder.layer.5.output.dense.bias', 'bert.sbert.encoder.layer.5.attention.self.query.bias', 'bert.sbert.encoder.layer.3.attention.self.value.weight', 'bert.sbert.encoder.layer.8.attention.output.dense.weight', 'bert.sbert.encoder.layer.8.attention.self.value.weight', 'bert.sbert.encoder.layer.7.attention.self.key.bias', 'bert.sbert.embeddings.word_embeddings.weight', 'bert.sbert.encoder.layer.8.output.LayerNorm.bias', 'bert.sbert.encoder.layer.2.output.dense.bias', 'bert.sbert.encoder.layer.9.attention.self.value.weight', 'bert.sbert.embeddings.token_type_embeddings.weight', 'bert.sbert.encoder.layer.3.attention.self.key.bias', 'bert.sbert.encoder.layer.1.output.dense.weight', 'bert.sbert.encoder.layer.8.intermediate.dense.weight', 'bert.sbert.encoder.layer.10.attention.self.value.bias', 'bert.sbert.encoder.layer.2.output.LayerNorm.bias', 'bert.sbert.encoder.layer.6.intermediate.dense.bias', 'bert.sbert.encoder.layer.10.attention.self.key.bias', 'bert.sbert.encoder.layer.1.output.LayerNorm.bias', 'bert.sbert.encoder.layer.9.intermediate.dense.weight', 'bert.sbert.encoder.layer.10.attention.output.dense.bias', 'bert.sbert.encoder.layer.1.attention.self.query.weight', 'bert.sbert.encoder.layer.3.attention.output.dense.bias', 'bert.sbert.encoder.layer.11.attention.self.query.bias', 'bert.sbert.encoder.layer.3.attention.output.dense.weight', 'bert.sbert.encoder.layer.5.output.LayerNorm.weight', 'bert.sbert.encoder.layer.1.output.dense.bias', 'bert.sbert.encoder.layer.2.output.LayerNorm.weight', 'bert.sbert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.sbert.encoder.layer.11.attention.self.value.weight', 'bert.sbert.encoder.layer.2.attention.self.query.bias', 'bert.sbert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.sbert.encoder.layer.9.attention.output.dense.bias', 'bert.sbert.encoder.layer.11.output.LayerNorm.bias', 'bert.sbert.encoder.layer.5.attention.self.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from bert_model import SBERT\n",
    "similarity_model = SBERT.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################### load functions ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433c58f4f09f4f2c9fd9de62bd24570d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d1a97b5f3549a48dbdf51755ac5fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3e1633920348aebfbff043902241cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertTokenizer, AdamW, BertConfig, BertForPreTraining\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "vocabs = tokenizer.get_vocab()\n",
    "seq_len = 300\n",
    "def tokenizze(data):\n",
    "    \n",
    "\n",
    "    # Load the BERT tokenizer.\n",
    "\n",
    "    # Training Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    target_labels = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for k, sent in enumerate(data):\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            str(sent),                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = seq_len,           # Pad & truncate all sentences.\n",
    "                            truncation=True,\n",
    "                            padding = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        \n",
    "        \n",
    "        tokens_ids=encoded_dict['input_ids'][0].cpu().numpy().copy()\n",
    "#         target_output = tokens.copy()\n",
    "\n",
    "        tokens_tensor = torch.tensor([tokens_ids])\n",
    "        input_ids.append(tokens_tensor)\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        target_labels.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        \n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    target_labels = torch.cat(target_labels, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, target_labels, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Lime for explainability\n",
    "import sklearn\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "label_names = [0, 1]\n",
    "# explainer = LimeTextExplainer(class_names=label_names)\n",
    "explainer = LimeTextExplainer(kernel_width = 25, class_names=label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# senten = X_test[0]\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "def predictor(texts):\n",
    "    result = []\n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                                str(text),                      # Sentence to encode.\n",
    "                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                max_length = seq_len,           # Pad & truncate all sentences.\n",
    "                                truncation=True,\n",
    "                                padding = True,\n",
    "                                return_attention_mask = True,   # Construct attn. masks.\n",
    "                                return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                           )\n",
    "\n",
    "         # Add the encoded sentence to the list.    \n",
    "        inputIds = encoded_dict['input_ids']\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attentionMask = encoded_dict['attention_mask']\n",
    "        labell = [1]\n",
    "        labell  = torch.tensor(labell)\n",
    "        bias_detector.eval()\n",
    "\n",
    "        inputIds = inputIds.to(device)\n",
    "        # inputIdss = inp6.to(device)\n",
    "        inputMask = attentionMask.to(device)\n",
    "        # inputMask = att6.to(device)\n",
    "        labell = labell.to(device)\n",
    "        with torch.no_grad():\n",
    "            (t_loss, t_logits) = bias_detector(inputIds, \n",
    "                                           token_type_ids=None, \n",
    "                                           attention_mask=inputMask,\n",
    "                                           labels=labell\n",
    "                                              )\n",
    "\n",
    "\n",
    "\n",
    "        t_logits = np.array(t_logits.cpu().numpy())\n",
    "\n",
    "        t_logits = [list(i) for i in t_logits]\n",
    "        t_logits = list(t_logits)\n",
    "\n",
    "        predictions = tf.nn.softmax(t_logits)\n",
    "        predictions = np.array(predictions)\n",
    "        predictions = [list(i) for i in predictions]\n",
    "        predictions = np.array(list(predictions[0]))\n",
    "        result.append(predictions)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_instance(text):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                            str(text),                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = seq_len,           # Pad & truncate all sentences.\n",
    "                            truncation=True,\n",
    "                            padding = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "     # Add the encoded sentence to the list.    \n",
    "    inputIds = encoded_dict['input_ids']\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attentionMask = encoded_dict['attention_mask']\n",
    "    labell = [1]\n",
    "    labell  = torch.tensor(labell)\n",
    "    bias_detector.eval()\n",
    "\n",
    "    inputIds = inputIds.to(device)\n",
    "    # inputIdss = inp6.to(device)\n",
    "    inputMask = attentionMask.to(device)\n",
    "    # inputMask = att6.to(device)\n",
    "    labell = labell.to(device)\n",
    "    with torch.no_grad():\n",
    "        (t_loss, t_logits) = bias_detector(inputIds, \n",
    "                                       token_type_ids=None, \n",
    "                                       attention_mask=inputMask,\n",
    "                                       labels=labell\n",
    "                                       )\n",
    "\n",
    "\n",
    "    return t_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/etokpoua/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/etokpoua/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "StopWords = stopwords.words(\"english\")\n",
    "\n",
    "def replace_word(text, findWord, replaceWord):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    return ' '.join(replaceWord if word.lower() == findWord.lower() else word for word in text_tokens)\n",
    "\n",
    "def mask_polarized_tokens(text):\n",
    "#     text = text.replace(\"\\n\", \" \")\n",
    "    original_text = text\n",
    "    text_tokens = word_tokenize(original_text)\n",
    "    num_samples = 1000\n",
    "    exp = explainer.explain_instance(text, predictor, num_features=10, num_samples=num_samples)\n",
    "    words = exp.as_list()\n",
    "    words = sorted(words, reverse=True, key=lambda x: x[1])\n",
    "    scores = [i[1] for i in words]\n",
    "    tokens_list = [i[0] for i in words]\n",
    "    important_words =[]\n",
    "    count = 0\n",
    "    for key, score in enumerate(scores):\n",
    "        if score>0.1:\n",
    "#         if ((score)>0.01) and (count<2) and (tokens_list[key] not in StopWords) and not tokens_list[key].isdigit():\n",
    "#             count = count + 1\n",
    "            important_words.append(tokens_list[key])\n",
    "        if len(important_words) == 0:\n",
    "            important_words.append(tokens_list[0])\n",
    "#     print(important_words)\n",
    "    text_tokens = ' '.join([\"[MASK]\" if word in important_words else word for word in text_tokens])\n",
    "    \n",
    "    return text_tokens, words\n",
    "#      if tokens_list[i] not in StopWords and not tokens_list[i].isdigit():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_wordpiece(wordpiece_tokens):\n",
    "    for key, i in reversed(list(enumerate(wordpiece_tokens))):\n",
    "        if i.startswith(\"##\"):\n",
    "#             print(i)\n",
    "#             print(\"\".join([wordpiece_tokens[key-1], wordpiece_tokens[key][2:]]))\n",
    "            wordpiece_tokens[key-1] = \"\".join([wordpiece_tokens[key-1], wordpiece_tokens[key][2:]])\n",
    "            del wordpiece_tokens[key]\n",
    "    return wordpiece_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mitigate(TEXT, source_TEXT):\n",
    "    inss = tokenizze([TEXT])\n",
    "    source_input = tokenizze([source_TEXT])\n",
    "\n",
    "    input_ids = inss[0].to(device)\n",
    "    attention_mask = inss[2].to(device)\n",
    "    source_labels = source_input[0].to(device)\n",
    "    labels_attention_mask = source_input[2].to(device)\n",
    "#     print(input_ids.shape) \n",
    "#     print(attention_mask.shape) \n",
    "#     print(source_labels.shape) \n",
    "#     print(labels_attention_mask.shape)\n",
    "\n",
    "    outputs = model(input_ids, \n",
    "                                     token_type_ids=None, \n",
    "                                     attention_mask=attention_mask,\n",
    "                                     labels=source_labels,\n",
    "                                     labels_attention_mask=labels_attention_mask, \n",
    "                   )\n",
    "\n",
    "    vocabs = tokenizer.get_vocab()\n",
    "    vocabs = dict((v,k) for k,v in vocabs.items())\n",
    " \n",
    "    import numpy as np\n",
    "    ids_list = []\n",
    "    new_text_list = []\n",
    "    pred_score = outputs[1][0].cpu().detach().numpy()\n",
    "    for outt in pred_score:\n",
    "        pred_flat = np.argmax(outt).flatten()\n",
    "        ids_list.append(np.argmax(outt))\n",
    "    # for ind in feature_importance_ind:\n",
    "        new_text_list.append(vocabs[pred_flat[0]])\n",
    "    #     print(vocabs[pred_flat[0]])\n",
    "    # labels_flat = np.array(labels_input_idss.flatten().cpu())\n",
    "    # logits_argmax = np.array([np.argmax(l, axis=0) for l in pred_score])\n",
    "    # pred_flat = logits_argmax.flatten()\n",
    "    # pred_flat = np.argmax(pred_score).flatten()\n",
    "    # print(pred_flat)\n",
    "    # print(labels_flat)\n",
    "    new_text_list = new_text_list[1:-1]\n",
    "    fix_wordpiece(new_text_list)\n",
    "    ids_list = torch.LongTensor([ids_list])\n",
    "#     print(inss)\n",
    "\n",
    "#     special_tokens = ['[CLS]', '[SEP]', '[PAD]']\n",
    "#     print(len(new_text_list))\n",
    "#     new_text_list = new_text_list[1:-1]\n",
    "#     special_tokens = ['[CLS]', '[SEP]', '[PAD]']\n",
    "#     new_text_list = new_text_list[:-1]\n",
    "\n",
    "#     resultwords  = [word for word in new_text_list if word not in special_tokens]\n",
    "    resultwords = new_text_list\n",
    "    new_text = ' '.join(resultwords)\n",
    "\n",
    "    attention_mask = attention_mask.cpu().detach()\n",
    "# #     print(new_text.shape)\n",
    "#     print(ids_list.shape)\n",
    "#     print(attention_mask.shape)\n",
    "    return new_text, ids_list, attention_mask\n",
    "    \n",
    "\n",
    "# print(np.sum(pred_flat == labels_flat) / len(labels_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################## Test with test data ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# def format_time(elapsed):\n",
    "#     '''\n",
    "#     Takes a time in seconds and returns a string hh:mm:ss\n",
    "#     '''\n",
    "#     # Round to the nearest second.\n",
    "#     elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "#     # Format as hh:mm:ss\n",
    "#     return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   # ========================================\n",
    "#               Testing\n",
    "# ========================================\n",
    "# After the completion of each training epoch, measure our performance on\n",
    "# our validation set.\n",
    "\n",
    "print(\"\")\n",
    "print(\"Running test...\")\n",
    "total_bias_score = 0\n",
    "total_similarity_score = 0\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.\n",
    "\n",
    "\n",
    "\n",
    "# Tracking variables \n",
    "total_eval_masked_accuracy = 0\n",
    "total_eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for ind, text in enumerate(X_test[610:1000]):\n",
    "   \n",
    "    #print(text, \"\\n\")\n",
    "    text_embedding = tokenizze([text])\n",
    "    text_logits = predict_instance(text)[0].cpu().numpy()\n",
    "    text_score = np.argmax(text_logits)\n",
    "#     print(text)\n",
    "    masked_TEXT, words = mask_polarized_tokens(text)\n",
    "#     print(masked_TEXT, \"\\n\")\n",
    "#     try:\n",
    "    count = count + 1\n",
    "    debiased_text, debiased_text_embedding, masked_attention_mask = mitigate(masked_TEXT, text)\n",
    "\n",
    "#     print(debiased_text_embedding)\n",
    "    #     calculate bias\n",
    "    bias_logits = predict_instance(debiased_text)[0].cpu().numpy()\n",
    "    bias_score = np.argmax(bias_logits)\n",
    "    total_bias_score = total_bias_score + bias_score\n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "#     calculate similarity\n",
    "    emb1 = similarity_model(text_embedding[0], token_type_ids=None, attention_mask=text_embedding[2])\n",
    "    emb2 = similarity_model(debiased_text_embedding, token_type_ids=None, attention_mask=masked_attention_mask)\n",
    "#     print(emb1.shape)\n",
    "    similarity = util.pytorch_cos_sim(emb1, emb2).item()\n",
    "    total_similarity_score = total_similarity_score + similarity\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"\\n ############# original text ##################### \\n\", text, \"############# score: \", text_score,  \"############# similarity: \", 1.000)\n",
    "    print(\"\\n# \\n\", words)\n",
    "    print(\"\\n ############### masked text ################### \\n\", masked_TEXT )\n",
    "    print(\"\\n ############## debiased text #################### \\n\", debiased_text, \"############# score: \", bias_score , \"############# similarity: \", similarity)\n",
    "  \n",
    "    \n",
    "    \n",
    "    with open(\"output/debiased_text.txt\", \"a\") as dd_text:\n",
    "            dd_text.write(f\"{debiased_text}\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
